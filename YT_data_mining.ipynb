{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractions données d'historique Youtube\n",
    "\n",
    "Le workflow suivant s'appuie sur les historiques de recherche et de vidéos visonnées. D'audtres données sont égallement disponibles mais non traité ici.\n",
    "Cette annalyse simple de données vise à montrer qu'il est facile d'extraire des informations et d'en apprendre plus sur les données de l'utilisateur.\n",
    "\n",
    "Les données qui nous intéressent sont dans le dossier YouTube et YouTube Music/historique. Deux fichiers se trouvent dans ce dossier:\n",
    "    * \"Historique des recherches\" : informations sur les recherches effectuées\n",
    "    * \"watch-history\": informations sur les vidéos visionnées\n",
    "Ces deux fichiers peuvent être télécharger au format html(défault) ou json. Pour faciliter l'extraction nous allons utiliser les fichiers json. \n",
    "\n",
    "Attention: les chemins et noms de fichiers vont varier en fonctions de où votre notebook se situe et de la langue de téléchargement (changement des nomns de dossier et fichiers).\n",
    "Ici les chemins sont pour des noms de dossiers en français avec un notebook situé au même niveau que le dossier Takeout.\n",
    "\n",
    "Des actions de la part de l'utilisateurs sont nécéssaire que dans les deux premières parties (import et chemins).\n",
    "Ensuite les résultats d'analyse seront directement visibles dans la partie résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports et installations nécéssaires\n",
    "\n",
    "Ci dessous se trouve tout les imports nécéssaires. Des installation peuvent être nécéssaires, vous trouverez en commentaire la ligne de commande pour réaliser l'installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tldextract #pip install tldextract\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk # pip install nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "#pip install keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chemins\n",
    "Ci-dessous remplacer les chemins par ceux correspondants pour vous. Si le fichier est bien chargé et au bon format un message le confirmant s'affichera, dans le cas contraire un message d'erreur s'affichera. \n",
    "Cela est la dernière étape où vous avez à intervenir. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chemin fichier \"Historique des recherches\"\n",
    "json_Hrecherche='Takeout_YT_json/YouTube et YouTube Music/historique/Historique des recherches.json'\n",
    "\n",
    "# chemin fichier \"Historique des vidéos regardés\" (watch)\n",
    "json_Hwatch='Takeout_YT_json/YouTube et YouTube Music/historique/watch-history.json'\n",
    "\n",
    "\n",
    "# Vérification: existance et format\n",
    "print(\"\\n\") \n",
    "\n",
    "    #recherche\n",
    "try:\n",
    " with open(json_Hrecherche): pass\n",
    " extension=os.path.splitext(json_Hrecherche)\n",
    " if \".json\" in extension[-1]:\n",
    "    print(\"Le fichier d'historique des recherches au format json est bien chargé.\\n\") \n",
    " else:\n",
    "    print(\"Erreur! Le fichier d'historique des recherches spécifié n'est pas au bon format. Le format attendu est json.\\n\")\n",
    "except IOError:\n",
    " print(\"Erreur! Le chemin indiqué pour le fichier d'historique des recherches spécificié n'est pas bon, le fichier est introuvable (cela peut venir du format de votre fichier).\\n\")\n",
    "\n",
    "    #watch\n",
    "try:\n",
    " with open(json_Hwatch): pass\n",
    " extension=os.path.splitext(json_Hwatch)\n",
    " if \".json\" in extension[-1]:\n",
    "    print(\"Le fichier d'historique des vidéos regardées au format json est bien chargé.\\n\") \n",
    " else:\n",
    "    print(\"Erreur! Le fichier d'historique des vidéos regardées spécifié n'est pas au bon format. Le format attendu est json.\\n\")\n",
    "except IOError:\n",
    " print(\"Erreur! Le chemin indiqué pour le fichier d'historique des vidéos regardées spécificié n'est pas bon, le fichier est introuvable (cela peut venir du format de votre fichier).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code analyse du fichier \"Historique des recherches\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création du data frame\n",
    "\n",
    "df_Hrecherche = pd.read_json(json_Hrecherche)\n",
    "\n",
    "# élimination du texte redondant ajouté devant les titres\n",
    "df_Hrecherche['title'] = df_Hrecherche['title'].str.replace(r'Vous avez recherché', '') \n",
    "# sauvegarde des recherches completes (car modifications ensuites)\n",
    "df_Hrecherche['title_comp']= df_Hrecherche['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Nettoyage des données \"title\", correspondent aux recherches effectuées\n",
    "##\n",
    "# conversion en minuscule\n",
    "df_Hrecherche['title'] = df_Hrecherche['title'].map(lambda x: x.lower())\n",
    "\n",
    "# retrait de la ponctuation (caractères spéciaux tels que $, ! ...)\n",
    "df_Hrecherche['title'] = df_Hrecherche['title'].map(lambda x: x.translate(x.maketrans('', '', string.punctuation)))\n",
    "\n",
    "# retrait des espaces vides inutiles \n",
    "df_Hrecherche['title'] = df_Hrecherche['title'].map(lambda x: x.strip())\n",
    "\n",
    "# conversion de la str en liste de \"tocken\" (nécessaire pour l'étape suivante)\n",
    "df_Hrecherche['title'] = df_Hrecherche['title'].map(lambda x: word_tokenize(x))\n",
    " \n",
    "# élimine les mots non-alphabétiques\n",
    "df_Hrecherche['title'] = df_Hrecherche['title'].map(lambda x: [word for word in x if word.isalpha()])\n",
    "\n",
    "# filtre les \"stop words\", mots considérés comme sans apport d'infomation (tel que and/et the/le-la ...)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words2 =set(stopwords.words(\"french\"))\n",
    "df_Hrecherche['title'] = df_Hrecherche['title'].map(lambda x: [w for w in x if not w in stop_words])\n",
    "df_Hrecherche['title'] = df_Hrecherche['title'].map(lambda x: [w for w in x if not w in stop_words2])\n",
    "\n",
    "# retransforme les liste en str\n",
    "df_Hrecherche['title'] = df_Hrecherche['title'].map(lambda x: ' '.join(x))\n",
    "\n",
    "\n",
    "# supprimer les lignes avec title vides\n",
    "df_Hrecherche.drop(df_Hrecherche.loc[df_Hrecherche['title']==''].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Hrecherche.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des dates au bon format pour des comparaison à suivre, année-mois-jour \n",
    "\n",
    "df_Hrecherche[\"year\"], df_Hrecherche[\"autre2\"]= df_Hrecherche[\"time\"].str.split(\"-\", 1 ).str # pour récupérer l'année\n",
    "df_Hrecherche[\"date\"], df_Hrecherche[\"autre\"] = df_Hrecherche[\"time\"].str.split(\"T\", 1).str\n",
    "df_Hrecherche[\"date\"]=pd.to_datetime(df_Hrecherche[\"date\"], errors = 'coerce')\n",
    "# Elimination des collones innutiles (nettoyage)\n",
    "df_Hrecherche=df_Hrecherche[['title','title_comp','year','date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation des valeurs manquantes dans les recherches effectuées, sont normalement à 0\n",
    "num_missing_desc = df_Hrecherche.isnull().sum()[2] \n",
    "print('Nombre de valeurs manquantes: ' + str(num_missing_desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations des valeurs années (year) en nombre car considérés comme str (due à la méthode de récupération)\n",
    "df_Hrecherche[['year']] = df_Hrecherche[['year']].apply(pd.to_numeric) \n",
    "#df_Hrecherche.dtypes # verification des types de chaque colonnes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ajout des jours de la semaine en fonction de la date\n",
    "df_Hrecherche[\"jour\"] = df_Hrecherche[\"date\"].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset de l'index, cette étape est crutiale pour la suite et le bon affichage des résultats\n",
    "# le formatage et l'élimination de certaines valeurs induisent des biais dans l'index et il est important de le réinitialiser\n",
    "df_Hrecherche=df_Hrecherche.reset_index()\n",
    "\n",
    "# Elimination de la collone index crée qui n'est pas utile\n",
    "df_Hrecherche=df_Hrecherche.drop(columns=[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupération des informations sur la première et dernière recherche\n",
    "dateHF= df_Hrecherche.iloc[-1,3] \n",
    "dateHF=dateHF.date()\n",
    "titleHF= df_Hrecherche.iloc[-1,1] \n",
    "dateHL= df_Hrecherche.iloc[1,3] \n",
    "dateHL=dateHL.date()\n",
    "titleHL= df_Hrecherche.iloc[1,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# réalistation d'un regroupement par année et jours de la semaine\n",
    "df_Hrecherche['nb'] = 1 # ajout d'un poid de 1 à chaque ligne\n",
    "summary_recherches = pd.DataFrame(df_Hrecherche.groupby(['year',\"jour\"]).count().astype('int')[\"nb\"]) \n",
    "summary_recherches = summary_recherches.unstack(level=0)\n",
    "summary_recherches = summary_recherches.fillna(0)\n",
    "\n",
    "\n",
    "# afficher les jours de la semaine dans le bon ordre\n",
    "jour = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "summary_recherches = summary_recherches.reindex(jour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig1=summary_recherches.plot(kind='line', subplots=False)\n",
    "#fig1.legend(loc='center left',bbox_to_anchor=(1.0, 0.5))\n",
    "#fig1.set_title(\"Nombre de recherche effectuées en fonction \\n des jours de la semaine et des années\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Représentation du nombre de recherche en fonction des années\n",
    "summary_recherchesA = pd.DataFrame(df_Hrecherche.groupby(['year']).count().astype('int')[\"nb\"]) \n",
    "\n",
    "#fig2=summary_recherchesA.plot(kind='bar', subplots=False).set_title(\"Nombre de recherche effectuées en fonction des années\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour représenter en fonction des jours seulement (années mergées)\n",
    "summary_recherchesJ = pd.DataFrame(df_Hrecherche.groupby(['jour']).count().astype('int')[\"nb\"]) \n",
    "# afficher les jours de la semaine dans le bon ordre\n",
    "jour = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "summary_recherchesJ = summary_recherchesJ.reindex(jour)\n",
    "\n",
    "#représentation graphique\n",
    "#fig3=summary_recherchesJ.plot(kind='line', subplots=False).set_title(\"Nombre de recherche effectuées en fonction des jours de la semaine\")\n",
    "#fig3=summary_recherchesJ.plot(kind='bar', subplots=False).set_title(\"Nombre de recherche effectuées en fonction des jours de la semaine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajout d'une colone avec les fréquences\n",
    "df_Hrecherche['freq']=df_Hrecherche.groupby(by='title')['title'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#par CHAINE DE CARACTERES\n",
    "# création avec les recherches et leurs count\n",
    "df_countR= df_Hrecherche['title'].value_counts().to_frame()\n",
    "df_countR=df_countR.reset_index()\n",
    "df_countR=df_countR.rename(columns = {'index': 'recherche' , 'title':'count'})\n",
    "df_countR=df_countR[~df_countR.recherche.str.contains(\"www\")]\n",
    "df_countR=df_countR.reset_index()\n",
    "df_countR=df_countR.drop(columns=[\"index\"])                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupération des 5 recherches es plus effectuées\n",
    "XrM=df_countR['recherche'].head( n=5)\n",
    "YrM=df_countR['count'].head( n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#par MOTS\n",
    "# séparation des recherches par mots et compte la fréquences de ces derniers\n",
    "wordC2 = df_Hrecherche.title.str.split(expand=True).stack().value_counts().to_frame()\n",
    "wordC2 =wordC2.reset_index()\n",
    "wordC2.columns = [\"word\",'count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupération des infomation des 5 mots les plus utilisés \n",
    "XrW=wordC2['word'].head( n=5)\n",
    "YrW=wordC2['count'].head( n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupération des mots dans une liste\n",
    "textR = df_Hrecherche.title.str.split().tolist()\n",
    "textR = [val for sublist in textR for val in sublist]\n",
    "textR = str(textR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# répupération du masque pour donner la forme au wordcloud\n",
    "yt_mask = np.array(Image.open( \"YT.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## création du wordcloud\n",
    "#wordcloud = WordCloud(max_words=200 ,mask=yt_mask, background_color=\"white\" , color_func=lambda *args, **kwargs: \"red\").generate(textR)\n",
    "#plt.figure()\n",
    "#plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "#plt.axis(\"off\")\n",
    "#plt.margins(x=0, y=0)\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code analyse du fichier \"watch_history\" (historique des vidéos regardées)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création du data frame\n",
    "\n",
    "df_Hwatch = pd.read_json(json_Hwatch)\n",
    "# élimination du texte redondant ajouté devant les title\n",
    "df_Hwatch['title'] = df_Hwatch['title'].str.replace(r'Vous avez regardé ', '')\n",
    "# colonne pour garder les titres sans modification de coté\n",
    "df_Hwatch['title_comp']= df_Hwatch['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "#Nettoyage des données \"title\", correspondent aux recherches effectuées\n",
    "##\n",
    "# converstion en minuscule\n",
    "df_Hwatch['title'] = df_Hwatch['title'].map(lambda x: x.lower())\n",
    "\n",
    "# retrait de la ponctuation (charactères spéciaux tels que $, ! ...)\n",
    "df_Hwatch['title'] = df_Hwatch['title'].map(lambda x: x.translate(x.maketrans('', '', string.punctuation)))\n",
    "\n",
    "# retrait des espaces vides inutiles \n",
    "df_Hwatch['title'] = df_Hwatch['title'].map(lambda x: x.strip())\n",
    "\n",
    "# conversion de la str en liste de \"tocken\" (nécessaire pour l'étape suivante)\n",
    "df_Hwatch['title'] = df_Hwatch['title'].map(lambda x: word_tokenize(x))\n",
    " \n",
    "# élimine les mots non-alphabétiques\n",
    "df_Hwatch['title'] = df_Hwatch['title'].map(lambda x: [word for word in x if word.isalpha()])\n",
    "\n",
    "# filtre les \"stop words\", mots considérés comme sans apport d'infomation (tel que and/et the/le-la ...)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words2 =set(stopwords.words(\"french\"))\n",
    "df_Hwatch['title'] = df_Hwatch['title'].map(lambda x: [w for w in x if not w in stop_words])\n",
    "df_Hwatch['title'] = df_Hwatch['title'].map(lambda x: [w for w in x if not w in stop_words2])\n",
    "# retransforme la liste en str\n",
    "df_Hwatch['title'] = df_Hwatch['title'].map(lambda x: ' '.join(x))\n",
    "\n",
    "\n",
    "# supprimer les lignes avec title vides\n",
    "df_Hwatch.drop(df_Hwatch.loc[df_Hwatch['title']==''].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des dates au bon format pour des comparaison à suivre, année-mois-jour \n",
    "# autre méthode que précédement mais même résultat\n",
    "df_Hwatch[\"year\"], df_Hwatch[\"autre2\"]= df_Hwatch[\"time\"].str.split(\"-\", 1 ).str # pour récupérer l'année\n",
    "\n",
    "df_Hwatch[\"year\"], df_Hwatch[\"month\"], df_Hwatch[\"autre\"] = df_Hwatch[\"time\"].str.split(\"-\", 2).str\n",
    "df_Hwatch[\"day\"], df_Hwatch[\"autre2\"] = df_Hwatch[\"autre\"].str.split(\"T\", 2).str\n",
    "df_Hwatch['date'] = pd.to_datetime(df_Hwatch[['year', 'month', 'day']], errors = 'coerce')\n",
    "\n",
    "# Elimination des collones innutiles (nettoyage)\n",
    "df_Hwatch=df_Hwatch[[\"title\",\"titleUrl\",\"title_comp\",\"year\",\"date\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation des valeurs manquantes dans les recherches effectuées, sont normalement à 0\n",
    "num_missing_desc = df_Hwatch.isnull().sum()[0]    # nombre de valeurs dont la recherche est manquante (title)\n",
    "print('Nombre de valeurs manquantes: ' + str(num_missing_desc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations des valeurs années (year) en nombre car considérés comme str (due à la méthode de récupération)\n",
    "df_Hwatch[['year']] = df_Hwatch[['year']].apply(pd.to_numeric) \n",
    "#df_Hwatch.dtypes # la date est bien au bon format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ajout des jours de la semaine\n",
    "df_Hwatch[\"jour\"] = df_Hwatch[\"date\"].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suppréssion des lignes avec vidéos supprimées\n",
    "df_Hwatch=df_Hwatch[~df_Hwatch.title.str.contains(\"vidéo supprimée\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset de l'index\n",
    "df_Hwatch=df_Hwatch.reset_index() # pour etre sure de l'ordre de l'index\n",
    "df_Hwatch=df_Hwatch.drop(columns=[\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupération d'informationss sur le première et dernière vidéo regardée\n",
    "dateHwF= df_Hwatch.iloc[-1,4] \n",
    "dateHwF=dateHwF.date()\n",
    "titleHwF= df_Hwatch.iloc[-1,2] \n",
    "lienHwF=df_Hwatch.iloc[-1,1]\n",
    "\n",
    "dateHwL= df_Hwatch.iloc[1,4] \n",
    "dateHwL=dateHwL.date()\n",
    "titleHwL= df_Hwatch.iloc[1,2] \n",
    "lienHwL=df_Hwatch.iloc[1,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# réalistation d'un regroupement par année et jours de la semaine\n",
    "df_Hwatch['nb'] = 1\n",
    "summary_watch = pd.DataFrame(df_Hwatch.groupby(['year',\"jour\"]).count().astype('int')[\"nb\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_watch = summary_watch.unstack(level=0)\n",
    "summary_watch = summary_watch.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le volume de données étant beaucoup plus important on a moins d'historique de vidéo reagrdé que de vidéo recherchées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# afficher les jours de la semaine dans le bon ordre\n",
    "jour = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "summary_watch = summary_watch.reindex(jour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Représentation du nombre de recherche en fonction des années et des jours de la semaine\n",
    "#fig5=summary_watch.plot(kind='line', subplots=False)\n",
    "#fig5.legend(loc='center left',bbox_to_anchor=(1.0, 0.5))\n",
    "#fig5.set_title(\"Nombre de vidéos regardées en fonction \\n des années et des jours de la semaine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A l'échelle des année\n",
    "summary_watchA = pd.DataFrame(df_Hwatch.groupby(['year']).count().astype('int')[\"nb\"]) \n",
    "\n",
    "#fig6=summary_watchA.plot(kind='bar', subplots=False).set_title(\"Nombre de vidéos regardées en fonction des années\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A l'échelle des jours \n",
    "summary_watchJ = pd.DataFrame(df_Hwatch.groupby(['jour']).count().astype('int')[\"nb\"]) \n",
    "# afficher les jours de la semaine dans le bon ordre\n",
    "jour = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "summary_watchJ = summary_watchJ.reindex(jour)\n",
    "\n",
    "#fig7=summary_watchJ.plot(kind='bar', subplots=False).set_title(\"Nombre de vidéos regardées en fonction des jours de la semaine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajout du nombre de fois que chaque vidéo a été regardée \n",
    "df_Hwatch['freq']=df_Hwatch.groupby(by='title')['title'].transform('count')\n",
    "\n",
    "df_countW= df_Hwatch['title_comp'].value_counts().to_frame()\n",
    "df_countW=df_countW.reset_index()\n",
    "df_countW.columns = [\"titre\",'count']\n",
    "df_countW=df_countW[~df_countW.titre.str.contains(\"www\")] # suppression des vidéo dont on a pas le titre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Récupération des information des vidéo les plus regardées\n",
    "Xw=df_countW['titre'].head( n=5)\n",
    "Yw=df_countW['count'].head( n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos historiques peuvent en apprendre  beaucoup sur nous, cette petite analyse de vos historiques youtubes ne représente qu'une fraction de ce qu'il est possible d'en extraire.\n",
    "Pensez-vous connaitre vos habitudes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Est-ce que vous vous rappelez de votre première* et de votre dernière recherche?\n",
    "\n",
    "* première réfère ici à la première recherche référencée dans le fichier, elle peut différer de votre actuelle première recherche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Votre première recherche date du: {0}, et était: \\\"{1}\\\". \\n'.format(dateHF,titleHF))\n",
    "print('Votre dernière recherche date du: {0}, et était: \\\"{1}\\\".'.format(dateHL,titleHL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avez vous une idée de l'évolution du nombre de vos recherche?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2=summary_recherchesA.plot(kind='bar', subplots=False).set_title(\"Nombre de recherche effectuées en fonction des années\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Et de la répartition de ces dernières au cours d'une semaine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1=summary_recherches.plot(kind='line', subplots=False)\n",
    "fig1.legend(loc='center left',bbox_to_anchor=(1.0, 0.5))\n",
    "fig1.set_title(\"Nombre de recherche effectuées en fonction \\n des jours de la semaine et des années\")\n",
    "fig3=summary_recherchesJ.plot(kind='line', subplots=False).set_title(\"Nombre de recherche effectuées en fonction des jours de la semaine\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les figures ci dessus peuvent en dire beaucoup sur vous et vos habitudes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avez-vous une idée des recherches que vous avez éffectuées le plus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Vos recherches les plus fréquentes sont \\\"{0[0]}\\\": {1[0]} fois, \\\"{0[1]}\\\": {1[1]} fois, \\\"{0[2]}\\\": {1[2]} fois, \\\"{0[3]}\\\": {1[3]} fois, \\\"{0[4]}\\\": {1[4]} fois. '.format(XrM,YrM))\n",
    "print('\\nEt les mots que vous avez le plus utilisé dans vos recherches sont \\\"{0[0]}\\\": {1[0]} fois, \\\"{0[1]}\\\": {1[1]} fois, \\\"{0[2]}\\\": {1[2]} fois, \\\"{0[3]}\\\": {1[3]} fois, \\\"{0[4]}\\\": {1[4]} fois. '.format(XrW,YrW))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Est ce que vous vous y attendiez?\n",
    "\n",
    "### La figure ci dessous donne un apperçu plus global de vos recherches, seriez vous a l'aise si tout le monde pouvais y avoir accès?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# création du wordcloud\n",
    "wordcloud = WordCloud(mask=yt_mask, background_color=\"white\" , color_func=lambda *args, **kwargs: \"red\").generate(textR)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.margins(x=0, y=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pensez-vous mieux connaitres vos habitudes concernant le visionnage de vidéos?\n",
    "\n",
    "La plateforme Youtube propose des choix de vidéos,enchaine les vidéos,... Pensez vous que les résultats suivant serais les même si il n'y avait pas toute cette incitation mais simplement la recherche?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('La première vidéo que vous avez regardé date du: {0}, et était: \\\"{1}\\\". Pour la revoir allez sur ce lien:  \\\"{2}\\\" . \\n'.format(dateHwF,titleHwF,lienHwF))\n",
    "\n",
    "print('Dans les données que nous récupérons, il arrive que les données les plus ancienne de l\\'historique ne stocke pas le titre mais seulement le lien. Mais si vous le suivez vous pourrez la redécouvrir.')\n",
    "# il y a un moyen d'éliminer ces lignes la mais cela peut enlevr bcp de ligne pour un fichier de 13000 peut en faire perdre 2000\n",
    "print('\\nLa dernière vidéo que vous avez regardé date du: {0}, et était: \\\"{1}\\\".  Pour la revoir allez sur ce lien:  \\\"{2}\\\" .\\n'.format(dateHwL,titleHwL,lienHwL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avez vous une idée de l'évolution du nombre de vidéo regardée?\n",
    "Pensez vous que la courbe d'évolution soit la même que pour les recherches?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig6=summary_watchA.plot(kind='bar', subplots=False).set_title(\"Nombre de vidéos regardées en fonction des années\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme cela est peut etre le cas avec vos données on se rend compte que la profondeur temporelle de notre historique de vue est plus faible que pour l'historique de recherche. Cela viens de la très forte différence (multiplication par 10 en moyenne) entre le nombre de recherche et le nombre de vidéos regardées.\n",
    "A cause de cela les représentations graphiques ci dessus sont moins pertinentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quelles sont vos habitudes sur une semaine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig5=summary_watch.plot(kind='line', subplots=False)\n",
    "fig5.legend(loc='center left',bbox_to_anchor=(1.0, 0.5))\n",
    "fig5.set_title(\"Nombre de vidéos regardées en fonction \\n des années et des jours de la semaine\")\n",
    "fig7=summary_watchJ.plot(kind='line', subplots=False).set_title(\"Nombre de vidéos regardées en fonction des jours de la semaine\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connaissez-vous vos vidéos favorites?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Les vidéos que vous avez le plus regardé sont \\\"{0[0]}\\\": {1[0]} fois, \\\"{0[1]}\\\": {1[1]} fois, \\\"{0[2]}\\\": {1[2]} fois, \\\"{0[3]}\\\": {1[3]} fois, \\\"{0[4]}\\\": {1[4]} fois. '.format(Xw,Yw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela correspond-il à ce que vous imaginiez? \n",
    "\n",
    "La question pour l'analyse de vidéo YT n'est pas de trouver des choses confidentielles mais de voir des habitudes ou des préférences. Nous avons tous une part de secret que nous n'avons pas envie de partager.\n",
    "Seriez vous prêt à laisser l'accès à d'autre personnes à ces données? Cela est enfait déjà le cas, youtube premièrement pour vous proposer des choix de vidéo, des pub. Google également ont accès et utilise ces données, mais également d'autres services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données présentées sur vos données ne sont qu'une partie de qu'il est possible de voir, pour aller plus loin il est possible d'utiliser une API. Une API youtube (https://developers.google.com/youtube/v3/getting-started) est disponible et permet d'obtenir encore plus d'information, par exemple de classer les vidéos par groupes (musique, cuisine, sport, série,...). Son implémention est difficile à mettre en place (notament la récupération de la bonne API), et cela n'a pas été possible dans le délais de ce projet. Mais Des outils ont déjà été dévellopés par d'autres, donc si vous voulez aller encore plus loin sur vos données vous pouvez aller tester:\n",
    "*https://github.com/Jessime/youtube_history\n",
    "*https://towardsdatascience.com/explore-your-activity-on-youtube-with-r-how-to-analyze-and-visualize-your-personal-data-history-b171aca632bc\n",
    "En raison de problèmes d'API nous n'avons pas réussi (malgrés les bonnes explications) à utiliser les projet proposés.\n",
    "Attention les nom de fichier et de dossier sont différents en français et anglais!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
